{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sources:\n",
    "## audio preparation:\n",
    "* https://www.tensorflow.org/tutorials/audio/simple_audio\n",
    "* https://www.tensorflow.org/io/tutorials/audio\n",
    "## siamese network:\n",
    "* https://github.com/hlamba28/One-Shot-Learning-with-Siamese-Networks\n",
    "## misc:\n",
    "* https://gitlab.tu-berlin.de/dl4aed/dl4aed-lectures/blob/master/04-audio-preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps tensorflow-io==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.4.0\"\n",
    "#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "import tensorflow_io as tfio\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset using this neat keras function\n",
    "download_link = 'https://gitlab.tubit.tu-berlin.de/dl4aed/dl4aed-data/raw/master/TinyUrbanSound8k/TinyUrbanSound8k.tar.gz'\n",
    "filepath = tf.keras.utils.get_file(Path('./_data/TinyUrbanSound8k.tar.gz').resolve(),\n",
    "                                download_link,\n",
    "                                cache_subdir=Path('./_data/').resolve(),\n",
    "                                extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from: https://www.tensorflow.org/guide/gpu?hl=en#limiting_gpu_memory_growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob('_data/TinyUrbanSound8k/*/*/')\n",
    "classes = tf.constant(sorted(set([Path(f).parts[-1] for f in folders])))\n",
    "num_classes = tf.shape(classes)[0]\n",
    "\n",
    "power = 2.\n",
    "nfft = 2048\n",
    "stride = 256\n",
    "mels = 64\n",
    "top_db = 80.\n",
    "sr = 16000\n",
    "fmin = 0.\n",
    "fmax = sr/2.\n",
    "wav_normalize_scale = 2**15\n",
    "length = 1\n",
    "image_width = tf.cast(tf.math.ceil(length*sr/stride), dtype=tf.int32).numpy()\n",
    "\n",
    "def load_file(file_path):\n",
    "    _audio = tfio.audio.AudioIOTensor(file_path, dtype=tf.dtypes.int16)\n",
    "    return tf.cast(tf.squeeze(_audio.to_tensor(), axis=-1), dtype=tf.float32) / wav_normalize_scale, tf.cast(_audio.rate, dtype=tf.float32)\n",
    "\n",
    "def folder_name_to_one_hot(file_path):\n",
    "    # for example: _data/TinyUrbanSound8k/train/siren/157648-8-0-0_00.wav\n",
    "    label = tf.strings.split(file_path, sep=\"/\")[-2]\n",
    "    label_idx = tf.argmax(tf.cast(tf.equal(classes, label), tf.int32))\n",
    "\n",
    "    # get one hot encoded array\n",
    "    one_hot = tf.one_hot(label_idx, num_classes, on_value=None, off_value=None, axis=None, dtype=tf.float32, name=None)\n",
    "    return one_hot\n",
    "\n",
    "def get_mel(audio):\n",
    "    spectrogram = tfio.experimental.audio.spectrogram(audio, nfft=nfft, window=nfft, stride=stride)\n",
    "    # due to the bad validation of tfio.experimental.audio.melscale the sr has to be a python variable\n",
    "    mel_spectrograms = tfio.experimental.audio.melscale(spectrogram**power, rate=sr, mels=mels, fmin=fmin, fmax=fmax)\n",
    "    return tf.transpose(tfio.experimental.audio.dbscale(mel_spectrograms, top_db=top_db))\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # get ground truth from file_path string\n",
    "    one_hot = folder_name_to_one_hot(file_path)\n",
    "\n",
    "    # load audio data and transform\n",
    "    audio, _ = load_file(file_path)\n",
    "    audio = get_mel(audio)\n",
    "    audio = tf.expand_dims(audio, axis=-1)\n",
    "\n",
    "    return audio, one_hot\n",
    "\n",
    "def train_augment(x, y):\n",
    "    # TODO: add more augmentations\n",
    "    x = tf.transpose(x[:,:,0])\n",
    "    x = tfio.experimental.audio.freq_mask(x, param=10)\n",
    "    x = tfio.experimental.audio.time_mask(x, param=10)\n",
    "    x = tf.expand_dims(tf.transpose(x), axis=-1)\n",
    "    return tf.cast(x, dtype=tf.float32), y\n",
    "\n",
    "# Probably works, eventhough sometimes the images are the same and the label is \"false\"\n",
    "# -> i guess, there are duplicates in the sets\n",
    "# TODO: Rethink if we should only compare directly if it's the same track, or if we work with a class vector and reduce it depending on the class?\n",
    "# -> Remember: tf.math.reduce_all(tf.math.equal(y[0], y[0])\n",
    "def extract(x, y):\n",
    "    # is 50:50 always the right thing to compare either the same track or two different ones? --> parameterize \n",
    "    pred = tf.cast(tf.random.uniform(tf.cast([1],dtype=tf.int32), \n",
    "                                     minval=0, maxval=2, dtype=tf.dtypes.int32, seed=None, name=None), dtype=tf.bool)\n",
    "    def true_fn():\n",
    "        return (train_augment(x[0]), train_augment(x[0])), tf.cast(True, dtype=tf.bool)\n",
    "    def false_fn():\n",
    "        return (train_augment(x[0]), train_augment(x[1])), tf.cast(False, dtype=tf.bool)\n",
    "    return tf.cond(pred, true_fn=true_fn, false_fn=false_fn, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autotune computation\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# folder with the training data\n",
    "train_files = './_data/TinyUrbanSound8k/train/*/*.wav'\n",
    "\n",
    "# define a dataset of file paths\n",
    "train_dataset = tf.data.Dataset.list_files(train_files)\n",
    "# run the preprocessing via map\n",
    "train_dataset = train_dataset.map(load_and_preprocess_data, num_parallel_calls=AUTOTUNE).cache()\n",
    "# shuffle the data\n",
    "train_dataset = train_dataset.shuffle(buffer_size=4000)\n",
    "\n",
    "# create mix/matches\n",
    "train_dataset = train_dataset.batch(2)\n",
    "train_dataset = train_dataset.map(extract, num_parallel_calls=AUTOTUNE, deterministic=None)\n",
    "\n",
    "# batch examples\n",
    "train_dataset = train_dataset.batch(512)\n",
    "\n",
    "# prefetch\n",
    "train_dataset = train_dataset.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: think about the difficulty of the extraction during inference\n",
    "# TODO: think about common metrics and evaluation techniques (accuracy may not be relevant anymore)\n",
    "\n",
    "# folder with the evaluation data\n",
    "test_files = './_data/TinyUrbanSound8k/test/*/*.wav'\n",
    "\n",
    "# define a dataset of file paths\n",
    "test_dataset = tf.data.Dataset.list_files(test_files)\n",
    "# run the preprocessing via map\n",
    "test_dataset = test_dataset.map(load_and_preprocess_data, num_parallel_calls=AUTOTUNE).cache()\n",
    "\n",
    "# TODO: with a new metric, this step might be irrelevant (we're also augmenting the test data)\n",
    "# create mix/matches\n",
    "test_dataset = test_dataset.batch(2)\n",
    "test_dataset = test_dataset.map(extract, num_parallel_calls=AUTOTUNE, deterministic=None)\n",
    "\n",
    "# batch examples\n",
    "test_dataset = test_dataset.batch(512)\n",
    "# prefetch\n",
    "test_dataset = test_dataset.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# TODO: think how to properly normalize the layer, depending on the input (rather do it after it was read and not augmented)\n",
    "#norm_layer = preprocessing.Normalization()\n",
    "#norm_layer.adapt(train_dataset.map(lambda x, _: x))\n",
    "\n",
    "# create model architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=[mels,image_width,1]))\n",
    "#model.add(norm_layer)\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "model.add(tf.keras.layers.MaxPool2D((2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(tf.keras.layers.GlobalMaxPool2D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(4096, activation=\"sigmoid\"))\n",
    "\n",
    "# Define the tensors for the two input images\n",
    "left_input = tf.keras.Input(shape=[mels,image_width,1])\n",
    "right_input = tf.keras.Input(shape=[mels,image_width,1])\n",
    "\n",
    "# Generate the encodings (feature vectors) for the two images\n",
    "encoded_l = model(left_input)\n",
    "encoded_r = model(right_input)\n",
    "\n",
    "# Add a customized layer to compute the absolute difference between the encodings\n",
    "L1_layer = tf.keras.layers.Lambda(lambda left, right: tf.keras.backend.abs(left - right))\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "# Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "prediction = tf.keras.layers.Dense(1, activation='sigmoid')(L1_distance)\n",
    "\n",
    "# Connect the inputs with the outputs\n",
    "siamese_net = tf.keras.models.Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "print(siamese_net.summary())\n",
    "\n",
    "# compile model\n",
    "siamese_net.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "print(\"Training\")\n",
    "siamese_net.fit(train_dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating\")\n",
    "siamese_net.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some minor visual inspection\n",
    "from matplotlib import pyplot as plt\n",
    "for ((x1, x2), label) in test_dataset:\n",
    "    for x1_, x2_, label_ in zip(x1,x2,label):\n",
    "        plt.imshow(x1_)\n",
    "        plt.show()\n",
    "        plt.imshow(x2_)\n",
    "        plt.show()\n",
    "        print(label_.numpy())\n",
    "        print(tf.cast(tf.round(\n",
    "            siamese_net([tf.expand_dims(x1_, axis=0),tf.expand_dims(x2_, axis=0)])\n",
    "        ), dtype=tf.bool).numpy())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
